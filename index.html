<!DOCTYPE html>
<html lang="en" class="h-100">

<head>
    <meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="ML Talk">
	<meta name="author" content="Tata">
    <title>Weekly Talks | Subhankar Mishra's Lab</title>
    <link href="https://www.niser.ac.in/~smishra/css/smlab.css" rel="stylesheet" type="text/css" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
</head>

<body class=""vsc-initialized>
    <div class="container">
        <header class="item">
			<h1>Subhankar Mishra Lab Weekly Talks</h1>
		</header>
		<a href="https://www.niser.ac.in/~smishra/"> Subhankar Mishra Lab</a>
		<a href="https://www.niser.ac.in/scps/">School of Computer Science, NISER, Bhubaneswar</a>
		<br>
		People: Annada Prasad Behera, Subhankar Mishra
		<hr>
		<h3>2023 Weekly Lab Talks at E2 from 0930-1030 every week.</h3>
		<ol>
    		<li><strong>Rucha Bhalchandra Joshi</strong></li>
<ul>
<li>August 7th, 2023
<li><b>End-to-end object detection with Transformers</b>
<li><small>
We present a new method that views object detection as a
direct set prediction problem. Our approach streamlines the detection
pipeline, effectively removing the need for many hand-designed
components like a non-maximum suppression procedure or anchor
generation that explicitly encode our prior knowledge about the task.
The main ingredients of the new framework, called DEtection
TRansformer or DETR, are a set-based global loss that forces unique
predictions via bipartite matching, and a transformer encoder-decoder
architecture. Given a fixed small set of learned object queries, DETR
reasons about the relations of the objects and the global image
context to directly output the final set of predictions in parallel.
The new model is conceptually simple and does not require a
specialized library, unlike many other modern detectors. DETR
demonstrates accuracy and run-time performance on par with the
well-established and highly-optimized Faster RCNN baseline on the
challenging COCO object detection dataset. Moreover, DETR can be
easily generalized to produce panoptic segmentation in a unified
manner. We show that it significantly outperforms competitive
baselines. Training code and pretrained models are available at
https://github.com/facebookresearch/detr</small>
<li><a href="https://ai.meta.com/research/publications/end-to-end-object-detection-with-transformers/">Link</a>
    		</ul>

    		<li><strong>Rahul Vishwakarma</strong></li>
    		<ul>
<li>August 14th, 2023
<li><b>DT-Solver: Automated Theorem Proving with
Dynamic-Tree Sampling Guided by Proof-level
Value Function</b>
<li><small>
Recent advances in neural theorem-proving resort to large
language models and tree searches. When proving a theorem, a language
model advises single-step actions based on the current proving state
and the tree search finds a sequence of correct steps using actions
given by the language model. However, prior works often conduct
constant computation efforts for each proving state while ignoring
that the hard states often need more exploration than easy states.
Moreover, they evaluate and guide the proof search solely depending on
the current proof state instead of considering the whole proof
trajectory as human reasoning does. So we will discuss the work of the
paper DT-Solver: Automated Theorem Proving with Dynamic-Tree Sampling
Guided by Proof-level Value Function, which proposes a novel
Dynamic-Tree Driven Theorem Solver (DT-Solver) to accommodate general
theorems, by guiding the search procedure with state confidence and
proof-level values. Specifically, DT-Solver introduces a dynamic-tree
Monte-Carlo search algorithm, which dynamically allocates computing
budgets for different state confidences, guided by a new proof-level
value function to discover proof states that require substantial
exploration.</small>
<li><a href="https://aclanthology.org/2023.acl-long.706.pdf">Link</a>
<li><a href="./2023/rahul.pdf">Slides</a>
    		</ul>

    		<li><strong>Aritra Mukhopadhaya</strong></li>
<ul>
<li>August 21st, 2023</li>
<li><b>You Only Look Once: Unified, Real-Time Object Detection</b>
<li><small>
We present YOLO, a new approach to object detection. Prior work on
object detection repurposes classifiers to perform detection. Instead,
we frame object detection as a regression problem to spatially separated
bounding boxes and associated class probabilities. A single neural network
predicts bounding boxes and class probabilities directly from full images
in one evaluation. Since the whole detection pipeline is a single network,
it can be optimized end-to-end directly on detection performance. Our
unified architecture is extremely fast. Our base YOLO model processes
images in real-time at 45 frames per second. A smaller version of the
network, Fast YOLO, processes an astounding 155 frames per second while
still achieving double the mAP of other real-time detectors. Compared to
state-of-the-art detection systems, YOLO makes more localization errors
but is less likely to predict false positives on background. Finally,
YOLO learns very general representations of objects. It outperforms
other detection methods, including DPM and R-CNN, when generalizing from
natural images to other domains like artwork.</small>
<li><a href="https://pjreddie.com/darknet/yolo/">Link</a>
<li><a href="./2023/aritra.pdf">Slides</a>
</ul>

    		<li><strong>Rucha Bhalchandra Joshi</strong></li>
<ul>
<li>August 7th, 2023
<li><b>End-to-end object detection with Transformers</b>
<li><small>
We present a new method that views object detection as a
direct set prediction problem. Our approach streamlines the detection
pipeline, effectively removing the need for many hand-designed
components like a non-maximum suppression procedure or anchor
generation that explicitly encode our prior knowledge about the task.
The main ingredients of the new framework, called DEtection
TRansformer or DETR, are a set-based global loss that forces unique
predictions via bipartite matching, and a transformer encoder-decoder
architecture. Given a fixed small set of learned object queries, DETR
reasons about the relations of the objects and the global image
context to directly output the final set of predictions in parallel.
The new model is conceptually simple and does not require a
specialized library, unlike many other modern detectors. DETR
demonstrates accuracy and run-time performance on par with the
well-established and highly-optimized Faster RCNN baseline on the
challenging COCO object detection dataset. Moreover, DETR can be
easily generalized to produce panoptic segmentation in a unified
manner. We show that it significantly outperforms competitive
baselines. Training code and pretrained models are available at
https://github.com/facebookresearch/detr</small>
<li><a href="https://ai.meta.com/research/publications/end-to-end-object-detection-with-transformers/">Link</a>
<li><a href="./2023/rucha.pdf">Slides</a>
    		</ul>

    		<li><strong>Annada Prasad Behera</strong></li>
    		<ul>
        		<li>September 4th, 2023</li>
    		</ul>
    		<li><strong>Jyotirmaya Shivottam</strong></li>
    		<ul>
        		<li>September 11th, 2023</li>
    		</ul>
		</ol>
		<h3>2022 - Paper Presentations</h3>
		<ol>
			<li>
			<strong>Talk 4 - Arindrima</strong>
			<small>
				<ul> <li>
            		 Date: Wednesday, Sept 21, 2022| 4:30 PM IST
				</li> <li>
						Title: TBA
				</li> </ul>
			</small>
    		</li>
			<li>

				<strong>Talk 3 - Annada Behera</strong>
				<small>
					<ul>
						<li>
			
							Date: Wednesday, Sept 7th, 2022| 4:30 PM IST
						</li>
						<li>
			
							Title: Handling position and visibility discontinuities for physically-based differentiable rendering.
						</li>
						<li>
			
							Abstract: 
						</li>
						<li>
			
							Slides: <a href="https://github.com/smlab-niser/labtalk/2022/slides/talk3.pdf">slides</a>
						</li>
						<li>
			
							Reference Paper(s): <a href="https://people.csail.mit.edu/tzumao/diffrt/"> Paper 1 </a>
							<a href="http://rgl.epfl.ch/publications/Vicini2022SDF"> Paper 2</a>
						</li>
					</ul>
				</small>
			</li>
			<li>

				<strong>Talk 2 - Jyotirmaya Shivottam</strong>
				<small>
					<ul>
						<li>
			
							Date: Wednesday, Aug 31st, 2022| 4:30 PM IST
						</li>
						<li>
			
							Title: Physics Aware Training <a href="https://www.nature.com/articles/s41586-021-04223-6">[nature]</a>
						</li>
						<li>
			
							Abstract: Deep-learning models have become pervasive tools in science and engineering. However, their energy requirements now increasingly limit their scalability. Deep-learning accelerators aim to perform deep learning energy-efficiently, usually targeting the inference phase and often by exploiting physical substrates beyond conventional electronics. Approaches so far have been unable to apply the backpropagation algorithm to train unconventional novel hardware in situ. The advantages of backpropagation have made it the de facto training method for large-scale neural networks, so this deficiency constitutes a major impediment. Here we introduce a hybrid in situâ€“in silico algorithm, called physics-aware training, that applies backpropagation to train controllable physical systems. Just as deep learning realizes computations with deep neural networks made from layers of mathematical functions, our approach allows us to train deep physical neural networks made from layers of controllable physical systems, even when the physical layers lack any mathematical isomorphism to conventional artificial neural network layers. To demonstrate the universality of our approach, we train diverse physical neural networks based on optics, mechanics and electronics to experimentally perform audio and image classification tasks. Physics-aware training combines the scalability of backpropagation with the automatic mitigation of imperfections and noise achievable with in situ algorithms. Physical neural networks have the potential to perform machine learning faster and more energy-efficiently than conventional electronic processors and, more broadly, can endow physical systems with automatically designed physical functionalities, for example, for robotics, materials and smart sensors.
						</li>
						<li>
			
							Slides: 
							<a href="https://github.com/smlab-niser/labtalk/tree/main/2022/slides/talk2.pdf">slides</a>
						</li>
						<li>
			
							Reference Paper(s): <a href="https://www.nature.com/articles/s41586-021-04223-6"> Link</a>
						</li>
					</ul>
				</small>
			</li>			
			<li>

				<strong>Talk 1 - Jyothish Kumar</strong>
				<small>
					<ul>
						<li>
			
							Date: Wednesday, Aug 24th, 2022| 4:30 PM IST
						</li>
						<li>
			
							Title: Some recent trends in prosthetics
						</li>
						<li>
			
							Abstract: In continuation with our ongoing research and analysis of various actuation and control strategies for robotic prosthesis, this is the second iteration appends the talk on current state of robotic prosthetics and soft robotics. Our discussion begins with understanding prosthetics and the scope of this term followed by a discussion into neuroprosthetics. Nervous tissue typically has a very low potential for intrinsic regeneration with the primary reason being that the connections between neurons is as important for the function as the gross number of neuron in the tissue. A lot of cases of physical disability are associated with damage to nervous tissue. A way to interact with these neurons has the potential to restore function in a disabled limb. On the other hand, a means of interaction with nervous tissue can also carve a way for development of advanced brain controlled prosthetics together with potential to associate a mechanical prosthetic limb as a contributor to the cognitive sensory bank by restoring sensations such as touch, pressure, temperature etc. in a limb thatâ€™s been replaced with a mechanical prosthesis. In this talk we discuss the various control strategies for robotic prostheses along with looking at some inspiring examples of neuroprosthetics based control and sensory rehabilitation in limb amputation cases. A short discussion on cognitive rehabilitation options associated with limb loss is also included.
						</li>
						<li>
			
							Slides: <a href="https://github.com/smlab-niser/labtalk/2022/slides/talk1.pdf">slides</a>
						</li>
						<li>
			
							Reference Paper(s): <a href="https://www.nature.com/articles/s41586-021-04223-6"> Link</a>
						</li>
					</ul>
				</small>
			</li>
		</ol>
		<hr>
		<h3>Past series</h3>
		<ul>

			<li>

				<a href="2021/ml_in_action.html">2021 - Machine Learning in Action</a>
			</li>
			<li>

				<a href="2021/paper_presentation.html">2021 - Paper Presentation</a>
			</li>
		</ul>        
    </div>
</body>

</html>
