<!DOCTYPE html>
<html lang="en" class="h-100">

<head>
    <meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="ML Talk">
	<meta name="author" content="Tata">
    <title>Weekly Talks | Subhankar Mishra's Lab</title>
    <link href="https://www.niser.ac.in/~smishra/css/smlab.css" rel="stylesheet" type="text/css" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
</head>

<body class=""vsc-initialized>
    <div class="container">
        <header class="item">
			<h1>Subhankar Mishra Lab Weekly Talks</h1>
		</header>
		<a href="https://www.niser.ac.in/~smishra/"> Subhankar Mishra Lab</a>
		<a href="https://www.niser.ac.in/scps/">School of Computer Science, NISER, Bhubaneswar</a>
		<br>
		People: Annada Prasad Behera, Subhankar Mishra
		<hr>
		<h3>2023 Weekly Lab Talks at E2 from 0930-1030 every week.</h3>
		<ol>
    		<li><strong>Rucha Bhalchandra Joshi</strong></li>
<ul>
<li>August 7th, 2023
<li><b>End-to-end object detection with Transformers</b>
<li><small>
We present a new method that views object detection as a
direct set prediction problem. Our approach streamlines the detection
pipeline, effectively removing the need for many hand-designed
components like a non-maximum suppression procedure or anchor
generation that explicitly encode our prior knowledge about the task.
The main ingredients of the new framework, called DEtection
TRansformer or DETR, are a set-based global loss that forces unique
predictions via bipartite matching, and a transformer encoder-decoder
architecture. Given a fixed small set of learned object queries, DETR
reasons about the relations of the objects and the global image
context to directly output the final set of predictions in parallel.
The new model is conceptually simple and does not require a
specialized library, unlike many other modern detectors. DETR
demonstrates accuracy and run-time performance on par with the
well-established and highly-optimized Faster RCNN baseline on the
challenging COCO object detection dataset. Moreover, DETR can be
easily generalized to produce panoptic segmentation in a unified
manner. We show that it significantly outperforms competitive
baselines. Training code and pretrained models are available at
https://github.com/facebookresearch/detr</small>
<li><a href="https://ai.meta.com/research/publications/end-to-end-object-detection-with-transformers/">Link</a>
    		</ul>

    		<li><strong>Rahul Vishwakarma</strong></li>
    		<ul>
<li>August 14th, 2023
<li><b>DT-Solver: Automated Theorem Proving with
Dynamic-Tree Sampling Guided by Proof-level
Value Function</b>
<li><small>
Recent advances in neural theorem-proving resort to large
language models and tree searches. When proving a theorem, a language
model advises single-step actions based on the current proving state
and the tree search finds a sequence of correct steps using actions
given by the language model. However, prior works often conduct
constant computation efforts for each proving state while ignoring
that the hard states often need more exploration than easy states.
Moreover, they evaluate and guide the proof search solely depending on
the current proof state instead of considering the whole proof
trajectory as human reasoning does. So we will discuss the work of the
paper DT-Solver: Automated Theorem Proving with Dynamic-Tree Sampling
Guided by Proof-level Value Function, which proposes a novel
Dynamic-Tree Driven Theorem Solver (DT-Solver) to accommodate general
theorems, by guiding the search procedure with state confidence and
proof-level values. Specifically, DT-Solver introduces a dynamic-tree
Monte-Carlo search algorithm, which dynamically allocates computing
budgets for different state confidences, guided by a new proof-level
value function to discover proof states that require substantial
exploration.</small>
<li><a href="https://aclanthology.org/2023.acl-long.706.pdf">Link</a>
<li><a href="./2023/rahul.pdf">Slides</a>
    		</ul>

    		<li><strong>Aritra Mukhopadhaya</strong></li>
<ul>
<li>August 21st, 2023</li>
<li><b>You Only Look Once: Unified, Real-Time Object Detection</b>
<li><small>
We present YOLO, a new approach to object detection. Prior work on
object detection repurposes classifiers to perform detection. Instead,
we frame object detection as a regression problem to spatially separated
bounding boxes and associated class probabilities. A single neural network
predicts bounding boxes and class probabilities directly from full images
in one evaluation. Since the whole detection pipeline is a single network,
it can be optimized end-to-end directly on detection performance. Our
unified architecture is extremely fast. Our base YOLO model processes
images in real-time at 45 frames per second. A smaller version of the
network, Fast YOLO, processes an astounding 155 frames per second while
still achieving double the mAP of other real-time detectors. Compared to
state-of-the-art detection systems, YOLO makes more localization errors
but is less likely to predict false positives on background. Finally,
YOLO learns very general representations of objects. It outperforms
other detection methods, including DPM and R-CNN, when generalizing from
natural images to other domains like artwork.</small>
<li><a href="https://pjreddie.com/darknet/yolo/">Link</a>
<li><a href="./2023/aritra.pdf">Slides</a>
</ul>

    		<li><strong>Rucha Bhalchandra Joshi</strong></li>
<ul>
<li>August 28th, 2023
<li><b>End-to-end object detection with Transformers</b>
<li><small>
We present a new method that views object detection as a
direct set prediction problem. Our approach streamlines the detection
pipeline, effectively removing the need for many hand-designed
components like a non-maximum suppression procedure or anchor
generation that explicitly encode our prior knowledge about the task.
The main ingredients of the new framework, called DEtection
TRansformer or DETR, are a set-based global loss that forces unique
predictions via bipartite matching, and a transformer encoder-decoder
architecture. Given a fixed small set of learned object queries, DETR
reasons about the relations of the objects and the global image
context to directly output the final set of predictions in parallel.
The new model is conceptually simple and does not require a
specialized library, unlike many other modern detectors. DETR
demonstrates accuracy and run-time performance on par with the
well-established and highly-optimized Faster RCNN baseline on the
challenging COCO object detection dataset. Moreover, DETR can be
easily generalized to produce panoptic segmentation in a unified
manner. We show that it significantly outperforms competitive
baselines. Training code and pretrained models are available at
https://github.com/facebookresearch/detr</small>
<li><a href="https://ai.meta.com/research/publications/end-to-end-object-detection-with-transformers/">Link</a>
<li><a href="./2023/rucha.pdf">Slides</a>
	</ul>

    		<li><strong>Jyotirmaya Shivottam</strong>
	<ul>
<li>September 12th, 2023
<li><b>Exploring Long-term (Time-)Series Forecasting (LTSF) using Echo
State Networks (ESNs) and comparisons with Single-Layer Perceptron
(SLP), MLP, LSTM and <emph>especially</emph> Attention-based methods.</b>
<li><small>ESNs are a type of Recurrent Neural Networks that are designed
to capture long-term dependencies in sequential, usually time-series,
data. Training the model involves training only the readout layer of
the network, while the internal weights are randomly initialized and
kept fixed. This makes the training process very fast and efficient,
enabling ESNs to be great online learners. The random initialization
of the internal weights is done in such a way that the network has the
so-called Echo State Property, which is responsible for the network's
ability to capture long-term dependencies, even in chaotic data. ESNs have
been shown to outperform all methods in LTSF, and are also much faster to
train. However, they are not as popular as LSTMs or now, transformers,
and are not as well-studied. In this talk, we will: 1. explore what <a
href="http://www.scholarpedia.org/article/Echo_state_network">ESNs
</a>are &amp; resources therein; 2. look at a recent ESN architecture - <a
href="https://dl.acm.org/doi/abs/10.1016/j.asoc.2023.110021">HP-MRESN</a>
3. compare them with LSTMs - several works & own experiments; 4. and,
in particular, compare them with recent results for attention-based
methods for LTSF.</small>
<li><a href="https://arxiv.org/abs/2205.13504">Link</a>
	</ul>

    		<li><strong>Jyotirmaya Shivottam</strong></li>
	<ul>
<li>September 22nd, 2023</li>
<li><b>Are Transformers Effective for Time Series Forecasting?</b>
<li><small>In this talk, we will discuss the aforementioned paper from
AAAI '23, which compares the effectiveness of transformer-based
architectures to simple linear (NN) models and concludes that linear
models outperform transformers in long-term time series forecasting
tasks. They present a hypothesis as to why this is the case and
provide a few suggestions for future work. We will discuss their
experiments and results. We will also briefly go over a recent set of
papers that use transformers, that have shown promising results in
long-term time series forecasting, though with some major caveats. We
will end with a discussion about a literature gap that exists in the
domain of (long-term) time series forecasting, pertaining to the lack
of comparison with ESN or even LSTM/GRU on these tasks.</small>
<li><a href="https://arxiv.org/abs/2205.13504">Link</a>
    </ul>

    		<li><strong>Annada Prasad Behera</strong></li>
	<ul>
<li>September 25th, 2023</li>
<li><b>Implicit Neural Representations with Periodic Activation Functions</b>
<li><small>Implicitly defined, continuous, differentiable signal
representations parameterized by neural networks have emerged as a
powerful paradigm, offering many possible benefits over conventional
representations. However, current network architectures for such
implicit neural representations are incapable of modeling signals with
fine detail, and fail to represent a signal's spatial and temporal
derivatives, despite the fact that these are essential to many
physical signals defined implicitly as the solution to partial
differential equations. We propose to leverage periodic activation
functions for implicit neural representations and demonstrate that
these networks, dubbed sinusoidal representation networks or SIRENs,
are ideally suited for representing complex natural signals and their
derivatives. We analyze SIREN activation statistics to propose a
principled initialization scheme and demonstrate the representation of
images, wavefields, video, sound, and their derivatives. Further, we
show how SIRENs can be leveraged to solve challenging boundary value
problems, such as particular Eikonal equations (yielding signed
distance functions), the Poisson equation, and the Helmholtz and wave
equations. Lastly, we combine SIRENs with hypernetworks to learn
priors over the space of SIREN functions. </small>
<li><a href="https://proceedings.neurips.cc/paper/2020/hash/53c04118df112c13a8c34b38343b9c10-Abstract.html">Link</a>
	</ul>

    		<li><strong>Rahul Vishwakarma</strong></li>
	<ul>
<li>October 17th, 2023</li>
<li><b>Formal Mathematics Statement Curriculum Learning</b>
<li><small>This paper explore the use of expert iteration in the
context of language modeling applied to formal mathematics. And show
that at same compute budget, expert iteration, which mean proof search
interleaved with learning, dramatically outperforms proof search only.
Authors also observe that when applied to a collection of formal
statements of sufficiently varied difficulty, expert iteration is
capable of finding and solving a curriculum of increasingly difficult
problems, without the need for associated ground-truth proofs.</small>
<li><a href="https://arxiv.org/pdf/2202.01344.pdf">Link</a>
	</ul>

        	<li><strong>Annada Prasad Behera</strong></li>
	<ul>
<li>November 3rd, 2023</li>
<li><b>3D Gaussian Splatting for Real-Time Radiance Field Rendering</b>
<li><small><p>Radiance Field methods have recently revolutionized
novel-view synthesis of scenes captured with multiple photos or
videos. However, achieving high visual quality still requires neural
networks that are costly to train and render, while recent faster
methods inevitably trade off speed for quality. For unbounded and
complete scenes (rather than isolated objects) and 1080p resolution
rendering, no current method can achieve real-time display rates.

<p>We introduce three key elements that allow us to achieve
state-of-the-art visual quality while maintaining competitive training
times and importantly allow high-quality real-time (≥ 100 fps)
novel-view synthesis at 1080p resolution.

<p>First, starting from sparse points produced during camera calibration,
we represent the scene with 3D Gaussians that preserve desirable
properties of continuous volumetric radiance fields for scene
optimization while avoiding unnecessary computation in empty space;
Second, we perform interleaved optimization/density control of the 3D
Gaussians, notably optimizing anisotropic covariance to achieve an
accurate representation of the scene; Third, we develop a fast
visibility-aware rendering algorithm that supports anisotropic
splatting and both accelerates training and allows realtime rendering.
We demonstrate state-of-the-art visual quality and real-time rendering
on several established datasets.</small>
<li><a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/">Link</a>

</ol>
		<h3>2022 - Paper Presentations</h3>
		<ol>
			<li>
			<strong>Talk 4 - Arindrima</strong>
			<small>
				<ul> <li>
            		 Date: Wednesday, Sept 21, 2022| 4:30 PM IST
				</li> <li>
						Title: TBA
				</li> </ul>
			</small>
    		</li>
			<li>

				<strong>Talk 3 - Annada Behera</strong>
				<small>
					<ul>
						<li>
			
							Date: Wednesday, Sept 7th, 2022| 4:30 PM IST
						</li>
						<li>
			
							Title: Handling position and visibility discontinuities for physically-based differentiable rendering.
						</li>
						<li>
			
							Abstract: 
						</li>
						<li>
			
							Slides: <a href="https://github.com/smlab-niser/labtalk/2022/slides/talk3.pdf">slides</a>
						</li>
						<li>
			
							Reference Paper(s): <a href="https://people.csail.mit.edu/tzumao/diffrt/"> Paper 1 </a>
							<a href="http://rgl.epfl.ch/publications/Vicini2022SDF"> Paper 2</a>
						</li>
					</ul>
				</small>
			</li>
			<li>

				<strong>Talk 2 - Jyotirmaya Shivottam</strong>
				<small>
					<ul>
						<li>
			
							Date: Wednesday, Aug 31st, 2022| 4:30 PM IST
						</li>
						<li>
			
							Title: Physics Aware Training <a href="https://www.nature.com/articles/s41586-021-04223-6">[nature]</a>
						</li>
						<li>
			
							Abstract: Deep-learning models have become pervasive tools in science and engineering. However, their energy requirements now increasingly limit their scalability. Deep-learning accelerators aim to perform deep learning energy-efficiently, usually targeting the inference phase and often by exploiting physical substrates beyond conventional electronics. Approaches so far have been unable to apply the backpropagation algorithm to train unconventional novel hardware in situ. The advantages of backpropagation have made it the de facto training method for large-scale neural networks, so this deficiency constitutes a major impediment. Here we introduce a hybrid in situ–in silico algorithm, called physics-aware training, that applies backpropagation to train controllable physical systems. Just as deep learning realizes computations with deep neural networks made from layers of mathematical functions, our approach allows us to train deep physical neural networks made from layers of controllable physical systems, even when the physical layers lack any mathematical isomorphism to conventional artificial neural network layers. To demonstrate the universality of our approach, we train diverse physical neural networks based on optics, mechanics and electronics to experimentally perform audio and image classification tasks. Physics-aware training combines the scalability of backpropagation with the automatic mitigation of imperfections and noise achievable with in situ algorithms. Physical neural networks have the potential to perform machine learning faster and more energy-efficiently than conventional electronic processors and, more broadly, can endow physical systems with automatically designed physical functionalities, for example, for robotics, materials and smart sensors.
						</li>
						<li>
			
							Slides: 
							<a href="https://github.com/smlab-niser/labtalk/tree/main/2022/slides/talk2.pdf">slides</a>
						</li>
						<li>
			
							Reference Paper(s): <a href="https://www.nature.com/articles/s41586-021-04223-6"> Link</a>
						</li>
					</ul>
				</small>
			</li>			
			<li>

				<strong>Talk 1 - Jyothish Kumar</strong>
				<small>
					<ul>
						<li>
			
							Date: Wednesday, Aug 24th, 2022| 4:30 PM IST
						</li>
						<li>
			
							Title: Some recent trends in prosthetics
						</li>
						<li>
			
							Abstract: In continuation with our ongoing research and analysis of various actuation and control strategies for robotic prosthesis, this is the second iteration appends the talk on current state of robotic prosthetics and soft robotics. Our discussion begins with understanding prosthetics and the scope of this term followed by a discussion into neuroprosthetics. Nervous tissue typically has a very low potential for intrinsic regeneration with the primary reason being that the connections between neurons is as important for the function as the gross number of neuron in the tissue. A lot of cases of physical disability are associated with damage to nervous tissue. A way to interact with these neurons has the potential to restore function in a disabled limb. On the other hand, a means of interaction with nervous tissue can also carve a way for development of advanced brain controlled prosthetics together with potential to associate a mechanical prosthetic limb as a contributor to the cognitive sensory bank by restoring sensations such as touch, pressure, temperature etc. in a limb that’s been replaced with a mechanical prosthesis. In this talk we discuss the various control strategies for robotic prostheses along with looking at some inspiring examples of neuroprosthetics based control and sensory rehabilitation in limb amputation cases. A short discussion on cognitive rehabilitation options associated with limb loss is also included.
						</li>
						<li>
			
							Slides: <a href="https://github.com/smlab-niser/labtalk/2022/slides/talk1.pdf">slides</a>
						</li>
						<li>
			
							Reference Paper(s): <a href="https://www.nature.com/articles/s41586-021-04223-6"> Link</a>
						</li>
					</ul>
				</small>
			</li>
		</ol>
		<hr>
		<h3>Past series</h3>
		<ul>

			<li>

				<a href="2021/ml_in_action.html">2021 - Machine Learning in Action</a>
			</li>
			<li>

				<a href="2021/paper_presentation.html">2021 - Paper Presentation</a>
			</li>
		</ul>        
    </div>
</body>

</html>
