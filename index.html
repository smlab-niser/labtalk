<!DOCTYPE html>
<html lang="en" class="h-100">

<head>
    <meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="ML Talk">
	<meta name="author" content="Tata">
    <title>Weekly Talks | Subhankar Mishra's Lab</title>
    <link href="https://www.niser.ac.in/~smishra/css/smlab.css" rel="stylesheet" type="text/css" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
</head>

<body class=""vsc-initialized>
    <div class="container">
        <header class="item">
			<h1>Subhankar Mishra Lab Weekly Talks</h1>
		</header>
		<a href="https://www.niser.ac.in/~smishra/"> Subhankar Mishra Lab</a>
		<a href="https://www.niser.ac.in/scps/">School of Computer Science, NISER, Bhubaneswar</a>
		<br>
		People: Annada Prasad Behera, Subhankar Mishra
		<hr>
<h3>2024 Weekly Lab Talks at E2 from 1430-1530 every week.</h3>
<ol><li><strong>Annada Prasad Behera</strong>
<ul>
<li> January 22nd, 2024
<li><b>Nitty gritty details of the CUDA rasterizer used in Gaussian splatting.</b>
<li><small>
<p>This talk presents a detailed explanation for all
the parts of the Gaussian splatting rasterizer and how the
<code>cuda_rasterizer.cu</code> implements the theory behind
the Gaussian splatting. Spherical harmonics for view dependent color,
frustrum culling for the culling rasterizers in camera space, splatting
the Gaussians from 3D to 2D screen space, rendering the pixles, tiling
on GPU to improve performance, the covariance matrix and at the end most
important of all, the backward pass that enables optimizing algorithms
suck as Adam or SGD to optimize the Gaussians with respect to a given
input image and produce novel views of the scene are discussed.
</small>
</ul>

<ol><li><strong>Sagar Prakash Barad </strong>
<ul>
<li> February 2nd, 2024
<li><b>Mamba: Linear-Time Sequence Modeling with Selective State Spaces</b>
<li><small>
<p>The talk will begin with a primer on the basics of selective
state models, followed by an overview of the Mamba architecture. The
presentation will cover the key aspects of the model such as the scanning
operation, hardware-aware memory allocations, computations, and conclude
with an examination of the results presented in the paper.</small>
</ul>

<li><strong>Rucha Bhalchandra Joshi</strong>
<ul>
<li> February 16th, 2024
<li><b>Quaternion Graph Neural Networks</b>
<li><small>
<p>Recently, graph neural networks (GNNs) have become an important and
active research direction in deep learning. It is worth noting that
most of the existing GNN-based methods learn graph representations
within the Euclidean vector space. Beyond the Euclidean space,
learning representation and embeddings in hyper-complex space have
also shown to be a promising and effective approach. To this end, we
propose Quaternion Graph Neural Networks (QGNN) to learn graph
representations within the Quaternion space. As demonstrated, the
Quaternion space, a hyper-complex vector space, provides highly
meaningful computations and analogical calculus through Hamilton
product compared to the Euclidean and complex vector spaces. Our
QGNN obtains state-of-the-art results on a range of benchmark
datasets for graph classification and node classification. Besides,
regarding knowledge graphs, our QGNN-based embedding model achieves
state-of-the-art results on three new and challenging benchmark
datasets for knowledge graph completion</small>
<li> <a href="https://arxiv.org/abs/2008.05089">Link</a> </li>
</ul>


<li><strong>Jyothish Kumar J</strong>
<ul>
<li> April 5th, 2024
<li><b>A stopwatch to measure the lap-time of Light, and other crazy
methods of 3D perception. </b>
<li><small>
<p>Ever wondered how we are able to measure the Time-Of-Flight of
light? Well light is the fastest thing around here, and to measure how
much time it took for light to bounce back from distances as small as 10
cm the stopwatch itself must be clicked twice, faster then the speed of
light. So, clearly that's not how it's done. In this talk, we discuss
how the simple principles of physics and electronics from High-school
textbooks are utilized ingeniously to device contineous wave time of
flight (CW-ToF). Principles of Pulsed ToF (DTof) , Frequency Modulated ToF
(FMToF) will also be discussed for the sake of context establishment.
If time permits, Other crazy machines used for 3D  perception (such as
wiggle-stereoscopy etc.) will be discussed. </small>
</ul>


<li><strong>Aritra Mukhopadhaya</strong>
<ul>
<li> April 12th, 2024
<li><b>KS-Lottery: Finding Certified Lottery Tickets for
Multilingual Language Models</b>
<li><small>
<p>The lottery ticket hypothesis posits the existence of “winning
tickets” within a randomly initialized neural network. Do winning
tickets exist for LLMs in fine-tuning scenarios? How can we find such
winning tickets? In this paper, we propose KS-Lottery, a method to
identify a small subset of LLM parameters highly effective in multilingual
fine-tuning. Our key idea is to use KolmogorovSmirnov Test to analyze
the distribution shift of parameters before and after fine-tuning. We
further theoretically prove that KS-Lottery can find the certified winning
tickets in the embedding layer, fine-tuning on the found parameters is
guaranteed to perform as well as full fine-tuning. Comparing KS-Lottery
with other parameter-efficient tuning algorithms on translation tasks,
the experimental results show that KS-Lottery finds a much smaller set of
parameters for fine-tuning while achieving the comparable performance as
full finetuning LLM. Surprisingly, we find that fine-tuning 18 tokens’
embedding of LLaMA suffices to reach the fine-tuning translation
performance.</small>
</ul>

<li><strong>Rahul Vishwakarma</strong>
<ul>
<li> April 19th, 2024
<li><b>Neural Theorem Proving in Lean</b>
<li><small>
<p> Theorem proving is a fundamental task in mathematics. With the advent
of large language models (LLMs) and interactive theorem provers (ITPs)
like Lean, there has been growing interest in integrating LLMs and ITPs
to automate theorem proving. In this approach, the LLM generates proof
steps (tactics), and the ITP checks the applicability of the tactics at
the current goal. The two systems work together to complete the proof.

<p>In this talk, in the first half, I will discuss two recent advancements
in the field of neural theorem proving: AlphaGeometry: Solving olympiad
geometry without human demonstrations Graph2Tac: Learning hierarchical
representations of math concepts in theorem proving

<p>In the second half of the talk, I will discuss the progress of my
work on neural theorem proving in Lean. This includes data augmentation,
introduction of dynamic sampling methods, our public website for proving
mathematical statements with its API usage, porting of standard datasets
(ProofNet and MiniF2F) to Lean 4 to facilitate experiments in Lean 4,
and the development of a new tokenizer for Lean to make the tactic
generator model more efficient.
</small>
</ul>



</ol>

<h3>2023 Weekly Lab Talks at E2 from 0930-1030 every week.</h3>
<ol>
<li><strong>Rucha Bhalchandra Joshi</strong></li>
<ul>
<li>August 7th, 2023
<li><b>End-to-end object detection with Transformers</b>
<li><small>
We present a new method that views object detection as a
direct set prediction problem. Our approach streamlines the detection
pipeline, effectively removing the need for many hand-designed
components like a non-maximum suppression procedure or anchor
generation that explicitly encode our prior knowledge about the task.
The main ingredients of the new framework, called DEtection
TRansformer or DETR, are a set-based global loss that forces unique
predictions via bipartite matching, and a transformer encoder-decoder
architecture. Given a fixed small set of learned object queries, DETR
reasons about the relations of the objects and the global image
context to directly output the final set of predictions in parallel.
The new model is conceptually simple and does not require a
specialized library, unlike many other modern detectors. DETR
demonstrates accuracy and run-time performance on par with the
well-established and highly-optimized Faster RCNN baseline on the
challenging COCO object detection dataset. Moreover, DETR can be
easily generalized to produce panoptic segmentation in a unified
manner. We show that it significantly outperforms competitive
baselines. Training code and pretrained models are available at
https://github.com/facebookresearch/detr</small>
<li><a href="https://ai.meta.com/research/publications/end-to-end-object-detection-with-transformers/">Link</a>
    		</ul>

    		<li><strong>Rahul Vishwakarma</strong></li>
    		<ul>
<li>August 14th, 2023
<li><b>DT-Solver: Automated Theorem Proving with
Dynamic-Tree Sampling Guided by Proof-level
Value Function</b>
<li><small>
Recent advances in neural theorem-proving resort to large
language models and tree searches. When proving a theorem, a language
model advises single-step actions based on the current proving state
and the tree search finds a sequence of correct steps using actions
given by the language model. However, prior works often conduct
constant computation efforts for each proving state while ignoring
that the hard states often need more exploration than easy states.
Moreover, they evaluate and guide the proof search solely depending on
the current proof state instead of considering the whole proof
trajectory as human reasoning does. So we will discuss the work of the
paper DT-Solver: Automated Theorem Proving with Dynamic-Tree Sampling
Guided by Proof-level Value Function, which proposes a novel
Dynamic-Tree Driven Theorem Solver (DT-Solver) to accommodate general
theorems, by guiding the search procedure with state confidence and
proof-level values. Specifically, DT-Solver introduces a dynamic-tree
Monte-Carlo search algorithm, which dynamically allocates computing
budgets for different state confidences, guided by a new proof-level
value function to discover proof states that require substantial
exploration.</small>
<li><a href="https://aclanthology.org/2023.acl-long.706.pdf">Link</a>
<li><a href="./2023/rahul.pdf">Slides</a>
    		</ul>

    		<li><strong>Aritra Mukhopadhaya</strong></li>
<ul>
<li>August 21st, 2023</li>
<li><b>You Only Look Once: Unified, Real-Time Object Detection</b>
<li><small>
We present YOLO, a new approach to object detection. Prior work on
object detection repurposes classifiers to perform detection. Instead,
we frame object detection as a regression problem to spatially separated
bounding boxes and associated class probabilities. A single neural network
predicts bounding boxes and class probabilities directly from full images
in one evaluation. Since the whole detection pipeline is a single network,
it can be optimized end-to-end directly on detection performance. Our
unified architecture is extremely fast. Our base YOLO model processes
images in real-time at 45 frames per second. A smaller version of the
network, Fast YOLO, processes an astounding 155 frames per second while
still achieving double the mAP of other real-time detectors. Compared to
state-of-the-art detection systems, YOLO makes more localization errors
but is less likely to predict false positives on background. Finally,
YOLO learns very general representations of objects. It outperforms
other detection methods, including DPM and R-CNN, when generalizing from
natural images to other domains like artwork.</small>
<li><a href="https://pjreddie.com/darknet/yolo/">Link</a>
<li><a href="./2023/aritra.pdf">Slides</a>
</ul>

    		<li><strong>Rucha Bhalchandra Joshi</strong></li>
<ul>
<li>August 28th, 2023
<li><b>End-to-end object detection with Transformers</b>
<li><small>
We present a new method that views object detection as a
direct set prediction problem. Our approach streamlines the detection
pipeline, effectively removing the need for many hand-designed
components like a non-maximum suppression procedure or anchor
generation that explicitly encode our prior knowledge about the task.
The main ingredients of the new framework, called DEtection
TRansformer or DETR, are a set-based global loss that forces unique
predictions via bipartite matching, and a transformer encoder-decoder
architecture. Given a fixed small set of learned object queries, DETR
reasons about the relations of the objects and the global image
context to directly output the final set of predictions in parallel.
The new model is conceptually simple and does not require a
specialized library, unlike many other modern detectors. DETR
demonstrates accuracy and run-time performance on par with the
well-established and highly-optimized Faster RCNN baseline on the
challenging COCO object detection dataset. Moreover, DETR can be
easily generalized to produce panoptic segmentation in a unified
manner. We show that it significantly outperforms competitive
baselines. Training code and pretrained models are available at
https://github.com/facebookresearch/detr</small>
<li><a href="https://ai.meta.com/research/publications/end-to-end-object-detection-with-transformers/">Link</a>
<li><a href="./2023/rucha.pdf">Slides</a>
	</ul>

    		<li><strong>Jyotirmaya Shivottam</strong>
	<ul>
<li>September 12th, 2023
<li><b>Exploring Long-term (Time-)Series Forecasting (LTSF) using Echo
State Networks (ESNs) and comparisons with Single-Layer Perceptron
(SLP), MLP, LSTM and <emph>especially</emph> Attention-based methods.</b>
<li><small>ESNs are a type of Recurrent Neural Networks that are designed
to capture long-term dependencies in sequential, usually time-series,
data. Training the model involves training only the readout layer of
the network, while the internal weights are randomly initialized and
kept fixed. This makes the training process very fast and efficient,
enabling ESNs to be great online learners. The random initialization
of the internal weights is done in such a way that the network has the
so-called Echo State Property, which is responsible for the network's
ability to capture long-term dependencies, even in chaotic data. ESNs have
been shown to outperform all methods in LTSF, and are also much faster to
train. However, they are not as popular as LSTMs or now, transformers,
and are not as well-studied. In this talk, we will: 1. explore what <a
href="http://www.scholarpedia.org/article/Echo_state_network">ESNs
</a>are &amp; resources therein; 2. look at a recent ESN architecture - <a
href="https://dl.acm.org/doi/abs/10.1016/j.asoc.2023.110021">HP-MRESN</a>
3. compare them with LSTMs - several works & own experiments; 4. and,
in particular, compare them with recent results for attention-based
methods for LTSF.</small>
<li><a href="https://arxiv.org/abs/2205.13504">Link</a>
	</ul>

    		<li><strong>Jyotirmaya Shivottam</strong></li>
	<ul>
<li>September 22nd, 2023</li>
<li><b>Are Transformers Effective for Time Series Forecasting?</b>
<li><small>In this talk, we will discuss the aforementioned paper from
AAAI '23, which compares the effectiveness of transformer-based
architectures to simple linear (NN) models and concludes that linear
models outperform transformers in long-term time series forecasting
tasks. They present a hypothesis as to why this is the case and
provide a few suggestions for future work. We will discuss their
experiments and results. We will also briefly go over a recent set of
papers that use transformers, that have shown promising results in
long-term time series forecasting, though with some major caveats. We
will end with a discussion about a literature gap that exists in the
domain of (long-term) time series forecasting, pertaining to the lack
of comparison with ESN or even LSTM/GRU on these tasks.</small>
<li><a href="https://arxiv.org/abs/2205.13504">Link</a>
    </ul>

    		<li><strong>Annada Prasad Behera</strong></li>
	<ul>
<li>September 25th, 2023</li>
<li><b>Implicit Neural Representations with Periodic Activation Functions</b>
<li><small>Implicitly defined, continuous, differentiable signal
representations parameterized by neural networks have emerged as a
powerful paradigm, offering many possible benefits over conventional
representations. However, current network architectures for such
implicit neural representations are incapable of modeling signals with
fine detail, and fail to represent a signal's spatial and temporal
derivatives, despite the fact that these are essential to many
physical signals defined implicitly as the solution to partial
differential equations. We propose to leverage periodic activation
functions for implicit neural representations and demonstrate that
these networks, dubbed sinusoidal representation networks or SIRENs,
are ideally suited for representing complex natural signals and their
derivatives. We analyze SIREN activation statistics to propose a
principled initialization scheme and demonstrate the representation of
images, wavefields, video, sound, and their derivatives. Further, we
show how SIRENs can be leveraged to solve challenging boundary value
problems, such as particular Eikonal equations (yielding signed
distance functions), the Poisson equation, and the Helmholtz and wave
equations. Lastly, we combine SIRENs with hypernetworks to learn
priors over the space of SIREN functions. </small>
<li><a href="https://proceedings.neurips.cc/paper/2020/hash/53c04118df112c13a8c34b38343b9c10-Abstract.html">Link</a>
	</ul>

    		<li><strong>Rahul Vishwakarma</strong></li>
	<ul>
<li>October 17th, 2023</li>
<li><b>Formal Mathematics Statement Curriculum Learning</b>
<li><small>This paper explore the use of expert iteration in the
context of language modeling applied to formal mathematics. And show
that at same compute budget, expert iteration, which mean proof search
interleaved with learning, dramatically outperforms proof search only.
Authors also observe that when applied to a collection of formal
statements of sufficiently varied difficulty, expert iteration is
capable of finding and solving a curriculum of increasingly difficult
problems, without the need for associated ground-truth proofs.</small>
<li><a href="https://arxiv.org/pdf/2202.01344.pdf">Link</a>
	</ul>

        	<li><strong>Annada Prasad Behera</strong></li>
	<ul>
<li>November 3rd, 2023</li>
<li><b>3D Gaussian Splatting for Real-Time Radiance Field Rendering</b>
<li><small><p>Radiance Field methods have recently revolutionized
novel-view synthesis of scenes captured with multiple photos or
videos. However, achieving high visual quality still requires neural
networks that are costly to train and render, while recent faster
methods inevitably trade off speed for quality. For unbounded and
complete scenes (rather than isolated objects) and 1080p resolution
rendering, no current method can achieve real-time display rates.

<p>We introduce three key elements that allow us to achieve
state-of-the-art visual quality while maintaining competitive training
times and importantly allow high-quality real-time (≥ 100 fps)
novel-view synthesis at 1080p resolution.

<p>First, starting from sparse points produced during camera calibration,
we represent the scene with 3D Gaussians that preserve desirable
properties of continuous volumetric radiance fields for scene
optimization while avoiding unnecessary computation in empty space;
Second, we perform interleaved optimization/density control of the 3D
Gaussians, notably optimizing anisotropic covariance to achieve an
accurate representation of the scene; Third, we develop a fast
visibility-aware rendering algorithm that supports anisotropic
splatting and both accelerates training and allows realtime rendering.
We demonstrate state-of-the-art visual quality and real-time rendering
on several established datasets.</small>
<li><a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/">Link</a>

</ul> <li><strong>Sagar Prakash Barad</strong></li> <ul>
<li>November 6th, 2023
<li><b>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Network</b>
<small>
<li><p>Faster R-CNN is a unified end-to-end object detection network that
significantly improved object detection by introducing the concept
of a Region Proposal Network (RPN). The key idea behind it was to use
the RPN to generate region proposals, which are essentially potential
bounding boxes containing objects of interest, and the model then uses
these initial proposals for further classification and refinement of
the bounding boxes.The RPN itself is a fully convolutional network
that performs two tasks simultaneously: predicting object bounds
and objectness scores for each position. When trained end-to-end, the
model becomes capable of producing high-quality region proposals, which
are subsequently utilized by Fast R-CNN for object detection.Trained
on datasets like PASCAL VOC and MS COCO, Faster R-CNN has shown its
effectiveness in various object detection tasks.
</small>

</ol>
		<h3>2022 - Paper Presentations</h3>
		<ol>
			<li>
			<strong>Talk 4 - Arindrima</strong>
			<small>
				<ul> <li>
            		 Date: Wednesday, Sept 21, 2022| 4:30 PM IST
				</li> <li>
						Title: TBA
				</li> </ul>
			</small>
    		</li>
			<li>

				<strong>Talk 3 - Annada Behera</strong>
				<small>
					<ul>
						<li>
			
							Date: Wednesday, Sept 7th, 2022| 4:30 PM IST
						</li>
						<li>
			
							Title: Handling position and visibility discontinuities for physically-based differentiable rendering.
						</li>
						<li>
			
							Abstract: 
						</li>
						<li>
			
							Slides: <a href="https://github.com/smlab-niser/labtalk/2022/slides/talk3.pdf">slides</a>
						</li>
						<li>
			
							Reference Paper(s): <a href="https://people.csail.mit.edu/tzumao/diffrt/"> Paper 1 </a>
							<a href="http://rgl.epfl.ch/publications/Vicini2022SDF"> Paper 2</a>
						</li>
					</ul>
				</small>
			</li>
			<li>

				<strong>Talk 2 - Jyotirmaya Shivottam</strong>
				<small>
					<ul>
						<li>
			
							Date: Wednesday, Aug 31st, 2022| 4:30 PM IST
						</li>
						<li>
			
							Title: Physics Aware Training <a href="https://www.nature.com/articles/s41586-021-04223-6">[nature]</a>
						</li>
						<li>
			
							Abstract: Deep-learning models have become pervasive tools in science and engineering. However, their energy requirements now increasingly limit their scalability. Deep-learning accelerators aim to perform deep learning energy-efficiently, usually targeting the inference phase and often by exploiting physical substrates beyond conventional electronics. Approaches so far have been unable to apply the backpropagation algorithm to train unconventional novel hardware in situ. The advantages of backpropagation have made it the de facto training method for large-scale neural networks, so this deficiency constitutes a major impediment. Here we introduce a hybrid in situ–in silico algorithm, called physics-aware training, that applies backpropagation to train controllable physical systems. Just as deep learning realizes computations with deep neural networks made from layers of mathematical functions, our approach allows us to train deep physical neural networks made from layers of controllable physical systems, even when the physical layers lack any mathematical isomorphism to conventional artificial neural network layers. To demonstrate the universality of our approach, we train diverse physical neural networks based on optics, mechanics and electronics to experimentally perform audio and image classification tasks. Physics-aware training combines the scalability of backpropagation with the automatic mitigation of imperfections and noise achievable with in situ algorithms. Physical neural networks have the potential to perform machine learning faster and more energy-efficiently than conventional electronic processors and, more broadly, can endow physical systems with automatically designed physical functionalities, for example, for robotics, materials and smart sensors.
						</li>
						<li>
			
							Slides: 
							<a href="https://github.com/smlab-niser/labtalk/tree/main/2022/slides/talk2.pdf">slides</a>
						</li>
						<li>
			
							Reference Paper(s): <a href="https://www.nature.com/articles/s41586-021-04223-6"> Link</a>
						</li>
					</ul>
				</small>
			</li>			
			<li>

				<strong>Talk 1 - Jyothish Kumar</strong>
				<small>
					<ul>
						<li>
			
							Date: Wednesday, Aug 24th, 2022| 4:30 PM IST
						</li>
						<li>
			
							Title: Some recent trends in prosthetics
						</li>
						<li>
			
							Abstract: In continuation with our ongoing research and analysis of various actuation and control strategies for robotic prosthesis, this is the second iteration appends the talk on current state of robotic prosthetics and soft robotics. Our discussion begins with understanding prosthetics and the scope of this term followed by a discussion into neuroprosthetics. Nervous tissue typically has a very low potential for intrinsic regeneration with the primary reason being that the connections between neurons is as important for the function as the gross number of neuron in the tissue. A lot of cases of physical disability are associated with damage to nervous tissue. A way to interact with these neurons has the potential to restore function in a disabled limb. On the other hand, a means of interaction with nervous tissue can also carve a way for development of advanced brain controlled prosthetics together with potential to associate a mechanical prosthetic limb as a contributor to the cognitive sensory bank by restoring sensations such as touch, pressure, temperature etc. in a limb that’s been replaced with a mechanical prosthesis. In this talk we discuss the various control strategies for robotic prostheses along with looking at some inspiring examples of neuroprosthetics based control and sensory rehabilitation in limb amputation cases. A short discussion on cognitive rehabilitation options associated with limb loss is also included.
						</li>
						<li>
			
							Slides: <a href="https://github.com/smlab-niser/labtalk/2022/slides/talk1.pdf">slides</a>
						</li>
						<li>
			
							Reference Paper(s): <a href="https://www.nature.com/articles/s41586-021-04223-6"> Link</a>
						</li>
					</ul>
				</small>
			</li>
		</ol>
		<hr>
		<h3>Past series</h3>
		<ul>

			<li>

				<a href="2021/ml_in_action.html">2021 - Machine Learning in Action</a>
			</li>
			<li>

				<a href="2021/paper_presentation.html">2021 - Paper Presentation</a>
			</li>
		</ul>        
    </div>
</body>

</html>
